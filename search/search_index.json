{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Assignment 2: Using Evo2 embeddings to Predict the 3D Folding of Chromatin","text":""},{"location":"#overview","title":"Overview","text":"<p>The <code>DNALongBench</code> paper introduced a set of challenging problems in genomics that require understanding long-range dependencies in DNA. One of the most difficult tasks is predicting the 3D folding of chromatin (a \"contact map\") from a 1D DNA sequence of over 1 million base pairs.</p> <p></p> <p>In this assignment, you will use Evo2 (7B), a DNA foundation model, on this Contact Map Prediction task. Ideally we would like to finetune the whole 7B model for this task, however, for simplicity, we will use the embeddings of this model to train a lightweight prediction head that learns to map these complex DNA features into a final contact map.</p> <p>You will learn to manage a complex deep learning environment, use Weights &amp; Biases (<code>wandb</code>) for experiment tracking, and visually analyze the model's predictions to interpret what it has learned.</p>"},{"location":"#1-environment-setup","title":"1. Environment Setup","text":"<p>IMPORTANT Note 1: This assignment requires access to NVIDIA H100 GPUs (Since Evo2 requires device compute capability 8.9 or higher required for FP8 execution).  You must first request a GPU node on the cluster before setting up the Conda environments. This ensures that Conda can correctly detect the CUDA toolkit.</p> <p>IMPORTANT Note 2: This assignment needs at least 18GB for the evo2 environment, read this on how to get some free space on your PSC home directory.</p> <p>Example: Requesting a GPU Node (SLURM)</p> <p>A Note on HPC Etiquette: Please use short, interactive srun sessions (e.g., --time=0:40:00) for setup and debugging. For the final, long-running training, always submit a non-interactive batch (sbatch) job. This is the standard and most considerate way to use shared GPU resources. This practice ensures that valuable GPU time is reserved for active computation, benefiting all users on the cluster.</p> <pre><code>srun --partition=GPU-shared --gres=gpu:h100-80:1 --time=0:40:00 --account=cis250160p --pty bash\n</code></pre>"},{"location":"#evo2-environment-for-model-fine-tuning","title":"Evo2 Environment (for model fine-tuning)","text":"<p>You will need to run the following commmands everytime you attempt to train the model. <pre><code>module load anaconda3/2024.10-1\nmodule load cuda/12.4.0\n</code></pre></p> <p>Create a conda evnvironment named <code>evo2</code> with Python 3.12 and install the required packages.</p> <pre><code>conda create -n evo2 python=3.12 -y\nconda activate evo2\n\nconda install -c conda-forge transformer-engine-torch=2.3.0\npip install flash-attn==2.8.0.post2 --no-build-isolation\n\npip install evo2\npip install natsort\npip install tensorflow==2.17.0\npip install wandb\npip install matplotlib scipy\n</code></pre>"},{"location":"#fixing-a-version-issue-with-glibc-follow-these-instructions-carefully","title":"Fixing a version issue with glibc (follow these instructions carefully)","text":"<p>After setting up the environment, you will need to fix a dependency issue. Flash-attn package requires <code>GLIBC_2.32</code>, while PSC has the version 2.28.</p> <p>Follow the following steps mentioned in this github comment by <code>NewComer00</code>. </p> <ol> <li>Build polyfill-glibc with feat/single-threaded support</li> <li>Get the path to flash-attn's .so lib</li> <li>Get your current GLIBC version</li> <li>Patch flash-attn's .so lib</li> </ol> <p>After you perform these steps, in the evo2 environement try running this command <code>python3 -c \"import flash_attn\"</code>. If it succeeds without error, you are good to go! Now you have a conda environment that supports Evo2 and Flash-attn! </p> <p>This step is somewhat tricky, if you are having trouble at this step, make a post on edstem.</p>"},{"location":"#2-data-paths-and-huggingface-cache-dir","title":"2. Data paths and Huggingface cache dir","text":"<p>Great job getting the environment set up! Working through dependency issues is a key skill when setting up advanced deep learning models. You often need to spend time reading github issues, searching for similar questions people have faced, and try to narrow down the issue you face.</p>"},{"location":"#huggingface-cache-directory-for-model-weights","title":"Huggingface Cache Directory (for model weights)","text":"<p>We have downloaded the Evo2 7B model weights on PSC. Therefore run this command to make sure huggingface looks at the correct location for the model weights. Otherwise it will download the weights to your home directory which has limited space.</p> <pre><code>export HF_HOME=/ocean/projects/cis250160p/rhettiar\necho 'export HF_HOME=/ocean/projects/cis250160p/rhettiar' &gt;&gt; ~/.bashrc;\n</code></pre> <p>This line adds the environment variable to your <code>.bashrc</code> file, so it is set automatically in future sessions. After completing the assignment, remove this line from your <code>.bashrc</code>.</p>"},{"location":"#dataset-path","title":"Dataset Path","text":"<p>We have downloaded the Contact Map Prediction dataset and have placed it in the shared directory (<code>/ocean/projects/cis250160p/rhettiar/contact_map_prediction/</code>). Try navigating to that directory (<code>cd</code>) to check if you can see the dataset. </p> <p>The scripts that you will use for fintuning already has this path set as the default data path. Therefore, if you plan to run the scripts on a different cluster make sure to edit <code>data_path</code>.</p> <p>If you are running on PSC, the dataloader will automatically use this path to load the data. If you are running on a different machine, download the dataset from this link.</p> <p>Note: For reduced train time, we are only using <code>test-0.tfr</code> <code>train-0.tfr</code> <code>valid-0.tfr</code> from this contact map dataset.</p>"},{"location":"#3-finetuning-evaluation","title":"3. Finetuning &amp; Evaluation","text":"<p>Clone this assignemnt 2 repository to your working directory.</p> <pre><code>git clone https://github.com/GenAIBioMed/GenAIBioMedAssignment2\n</code></pre>"},{"location":"#a-integrate-with-wandb-for-experiment-tracking","title":"a) Integrate with <code>wandb</code> (for experiment tracking)","text":"<p>Your first task is to edit the finetuning code (<code>finetune_contact_map.py</code>). Fill in the sections marked <code>TODO</code> to integrate Weights &amp; Biases (<code>wandb</code>) for experiment tracking. Then run finetuning with <code>python finetune_contact_map.py</code>. You can refer to this documentation when learning about wandb Getting Started with Weights &amp; Biases.</p>"},{"location":"#b-run-finetuning-evaluation","title":"b) Run Finetuning &amp; Evaluation","text":"<p>Activate your <code>evo2</code> environment and run the fine-tuning script. The script is configured to use the evo2_7b model by default. The lines below loads cuda and sets the huggingface cache path to the shared folder. While you can run this through an interactive job, we recommend running as a sbatch job so that the chance of a sudden interruption is low.</p> <pre><code>conda activate evo2\nmodule load cuda/12.4.0\nexport HF_HOME=/ocean/projects/cis250160p/rhettiar\npython finetune_contact_map.py\n</code></pre> <p>After training is complete, use your best model checkpoint to run the evaluation script (this will saved model predictions and groundtruth as .npy files).</p> <pre><code>conda activate evo2\nmodule load cuda/12.4.0\nexport HF_HOME=/ocean/projects/cis250160p/rhettiar\npython evaluate_contact_map.py\n</code></pre>"},{"location":"#c-towards-better-performance","title":"c) Towards better performance","text":"<p>Above we have only trained for 10 epochs, and for a subset of data: <code>test-0.tfr</code> <code>train-0.tfr</code> <code>valid-0.tfr</code>. Try out one of the following to see how you might improve the prediction head's performance. </p> <ul> <li>Training for more epochs</li> <li>Improving the prediction head architecture</li> <li>Including more data (all train-*.tfr files). (You will need to download more files, e.g. train-1.tfr, train-2.tfr and then copy them to your scratch folder -- <code>/ocean/projects/cis250160p/&lt;username&gt;</code>, then update the path variable in the scripts)</li> </ul> <p>(\u26a0\ufe0f make sure to change the checkpoint saving path -- otherwise you might overwrite the default checkpoint that you trained. Same goes for the prediction saving dir as well! you may want to rename the <code>npy</code> files that you already obtained)</p>"},{"location":"#d-visualize-predictions-analyze-performance","title":"d) Visualize Predictions &amp; Analyze Performance","text":"<p>The <code>evaluate_contact_map.py</code> script saves the raw prediction and target data. You will create a new Python script (e.g., <code>analyze_performance.py</code>) to process these files.</p> <p>Your script must perform two key tasks:</p> <ol> <li> <p>Calculate Overall Performance:</p> <ul> <li>Load the <code>pred.npy</code> and <code>target.npy</code> files.</li> <li>Iterate through every sample in the test set. For each sample, calculate the Pearson (PCC) between the predicted and the ground truth contact map.</li> <li>Compute the average PCC across the entire test set. These values represent your model's overall performance.</li> </ul> </li> <li> <p>Generate a Representative Visualization:</p> <ul> <li>After calculating all scores, find a single genomic region from the test set that demonstrates good performance (e.g., its score is near or above the average).</li> <li>Generate a plot for this single example. The figure should contain two subplots: the Ground Truth map and your Finetuned Prediction.</li> <li>Use <code>matplotlib.pyplot.imshow</code> to display the 50x50 40x40 matrices.</li> <li>Clearly label each subplot with the specific PCC for that individual example.</li> </ul> </li> </ol>"},{"location":"#4-analysis-submission","title":"4. Analysis &amp; Submission","text":"<p>Compress the following into a single zip file for submission.</p> <ol> <li> <p>Modified Python Files:</p> <ul> <li><code>finetune_contact_map.py</code> (with <code>wandb</code> integration).</li> <li><code>analyze_performance.py</code> (the script you wrote for calculation and visualization).</li> </ul> </li> <li> <p>PDF Report: </p> <p>Your report must include the following for each of the two conditions: 1) default number of epochs, 2) one option you tried in section 3c (state what you tried).</p> <ul> <li>Report the final loss you achieve</li> <li> <p>A link to your public <code>wandb</code> experiment's report showing your training curves. Here's an example.</p> </li> <li> <p>Overall Performance Metrics: State the average PCC you calculated across the entire test set.</p> </li> <li>Representative Visualization: Include the visualization figure you generated for a single, high-performing example. Ensure the subplots are clearly labeled with their specific scores.</li> <li>Written Analysis: Your analysis should address the following points in a few short paragraphs:<ol> <li>Performance Comparison: Based on your average scores, does your fine-tuned Evo2 model achieve better or worse performance than the CNN baseline reported in the DNALongBench paper?</li> <li>Discussion: Whether it performs better or worse, discuss potential reasons for this outcome. What could have we done in the finetuning script for better performance?</li> </ol> </li> </ul> </li> </ol>"},{"location":"disk_space/","title":"Note on Disk Space","text":""},{"location":"disk_space/#a-quick-note-on-disk-space","title":"A Quick Note on Disk Space","text":"<p>Your home directory has a limited storage quota. The Conda environment and the packages you install, especially large ones for deep learning, can consume several gigabytes. For instance the HW1 conda env (<code>progen</code>) will be 6.8GB, and the evo2 environment will be 18.2GB. So most certainly you might run into space issues.</p> <p>The <code>polyfill-glibc</code> patching step in the next section can fail with a misleading <code>Error writing to file</code> message if your disk is full.</p> <p>Before proceeding, it's a good idea to check your available space on your home folder in PSC. You can see how much space you have left on the filesystem with:</p> <pre><code>my_quotas\n# example output:\nThe quota for home directory /jet/home/rhettiar\nStorage quota: 25.00GiB\n Storage used: 22.82GiB\n  Inode quota: 0\n  Inodes used: 120,115\n\nThe quota for project directory /ocean/projects/cis250160p\nStorage quota: 1.95TiB\n Storage used: 608.20GiB\n  Inode quota: 12,140,000\n  Inodes used: 1,293,748\n</code></pre> <p>If you are low on space, here are three ways to fix it:</p> <ol> <li>Remove Old Conda Environments You may have environments from previous assignments that you no longer need. First, list all your environments to see what you have:</li> </ol> <pre><code>conda env list\n</code></pre> <p>If you find an old environment you don't need anymore (e.g., one named <code>progen</code> from a previous project), you can remove it completely to free up a lot of space.</p> <p><pre><code># Replace &lt;env_name&gt; with the actual name of the environment to delete\nconda env remove -n &lt;env_name&gt;\n\n# Example:\nconda env remove -n progen\n</code></pre> 2.  Clean Conda Caches: Conda keeps a cache of downloaded packages that you can safely clear to free up a significant amount of space.     <pre><code>conda clean --all\n</code></pre> 3.  Move Other Projects: If you have large files or old projects in your home directory, move them to your dedicated project scratch space, which has much more room.     <pre><code># Example: moving progen env\nmv /jet/home/&lt;your_username&gt;/.conda/envs/progen /ocean/projects/cis250160p/&lt;your_username&gt;/\n</code></pre></p>"}]}